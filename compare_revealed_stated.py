"""
LitmusValues Elo Rating Visualization Script (Revealed vs Stated Preferences)
==============================================================================
This script creates visualizations comparing AI models' REVEALED vs STATED value preferences:

REVEALED PREFERENCES (Behavioral):
- Calculated from actual choices in ethical dilemmas
- Shows what values the model DOES prioritize in practice
- Source: revealed_preferences/{model}_elo_ratings.csv
- Generated by: calculate_elo_unified.py (with PREFERENCE_TYPE='revealed')

STATED PREFERENCES (Self-Report):
- Calculated from direct questions asking which values are more important
- Shows what values the model SAYS it prioritizes
- Source: stated_preferences/{model}_elo_ratings.csv
- Generated by: calculate_elo_unified.py (with PREFERENCE_TYPE='stated')

WORKFLOW:
1. Collect data:
   - Revealed: run_ai_risk_dilemmas_commented.py
   - Stated: collect_stated_preferences_commented.py
2. Calculate ELOs:
   - Run calculate_elo_unified.py twice (once for each PREFERENCE_TYPE)
3. Compare and visualize:
   - Run this script (compare_revealed_stated.py)

Visualizations Created:
1. COMPARISON BAR CHART: Side-by-side Elo ratings for revealed vs stated
2. SCATTER PLOT: Correlation between revealed and stated preferences
3. DIFFERENCE CHART: Which values show biggest gaps between stated and revealed
4. RANK COMPARISON: How value rankings change from revealed to stated

This comparison reveals potential "value alignment gaps" - cases where what a model
says it values differs from what it actually prioritizes in decisions.

From the paper: "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values
Prioritization with AIRISKDILEMMAS" (https://arxiv.org/pdf/2505.14633)
"""

# ============================================================================
# IMPORTS
# ============================================================================

import os  # For file system operations (creating directories)

import pandas as pd  # For data manipulation
import plotly.express as px  # For creating interactive visualizations
from scipy.stats import spearmanr  # For Spearman's rank correlation


# ============================================================================
# CONFIGURATION SETTINGS
# ============================================================================

# Change working directory to the script's location
# This ensures all relative paths work correctly regardless of where you run the script from
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)

# Model: Name of the model to visualize (must match filename from evaluation)
#model = 'gpt-4.1-2025-04-14'
#model = 'gpt-4.1-mini'
model = 'gpt-4o-mini'

# Input directories
revealed_prefs_dir = "revealed_preferences"  # Revealed preferences (from behavioral dilemmas)
stated_prefs_dir = "stated_preferences"  # Stated preferences (from direct questions)

# Output directory for comparison visualizations
comparison_fig_dir = "comparison_figs"

# Create output directory if it doesn't exist
if not os.path.exists(comparison_fig_dir):
    os.makedirs(comparison_fig_dir)

# Define input file paths
revealed_prefs_file = f"{revealed_prefs_dir}/{model}_elo_ratings.csv"  # Revealed preferences Elo ratings
stated_prefs_file = f"{stated_prefs_dir}/{model}_elo_ratings.csv"  # Stated preferences Elo ratings

# Define output file paths
comparison_chart_path = f"{comparison_fig_dir}/{model}_revealed_vs_stated.png"
scatter_plot_path = f"{comparison_fig_dir}/{model}_correlation.png"
difference_chart_path = f"{comparison_fig_dir}/{model}_differences.png"
rank_comparison_path = f"{comparison_fig_dir}/{model}_rank_comparison.png"


# ============================================================================
# STEP 0: LOAD BOTH REVEALED AND STATED PREFERENCE ELO RATINGS
# ============================================================================

print(f"Loading Elo ratings for model: {model}\n")

# Load REVEALED preferences (from behavioral dilemmas)
print(f"Loading revealed preferences from: {revealed_prefs_file}")
revealed_df = pd.read_csv(revealed_prefs_file, index_col='Rank')
print(f"  ✓ Loaded {len(revealed_df)} value classes\n")

# Load STATED preferences (from direct questions)
print(f"Loading stated preferences from: {stated_prefs_file}")
stated_df = pd.read_csv(stated_prefs_file, index_col='Rank')
print(f"  ✓ Loaded {len(stated_df)} value classes\n")


# ============================================================================
# STEP 1: MERGE AND PREPARE DATA FOR COMPARISON
# ============================================================================

# Rename columns to distinguish between revealed and stated
# Both files now use 'Elo Rating' as the column name (from calculate_elo_unified.py)
revealed_df = revealed_df.rename(columns={'Elo Rating': 'revealed_elo', 'value_class': 'value'})
stated_df = stated_df.rename(columns={'Elo Rating': 'stated_elo', 'value_class': 'value'})

# Merge the two dataframes on value name
comparison_df = pd.merge(
    revealed_df[['value', 'revealed_elo']],
    stated_df[['value', 'stated_elo']],
    on='value',
    how='outer'  # Keep all values even if they don't appear in both
)

# Calculate the difference (stated - revealed)
# Positive = stated higher than revealed (says it's more important than acts)
# Negative = revealed higher than stated (acts like it's more important than says)
comparison_df['difference'] = comparison_df['stated_elo'] - comparison_df['revealed_elo']
comparison_df['abs_difference'] = comparison_df['difference'].abs()

# Calculate ranks for both revealed and stated preferences
# Rank 1 = highest Elo rating (most prioritized)
comparison_df['revealed_rank'] = comparison_df['revealed_elo'].rank(ascending=False, method='min').astype(int)
comparison_df['stated_rank'] = comparison_df['stated_elo'].rank(ascending=False, method='min').astype(int)
comparison_df['rank_change'] = comparison_df['revealed_rank'] - comparison_df['stated_rank']
# Positive rank_change = moved up in stated (became more important when asked)
# Negative rank_change = moved down in stated (became less important when asked)

# Sort by revealed Elo for consistent ordering
comparison_df = comparison_df.sort_values('revealed_elo', ascending=False).reset_index(drop=True)

print(f"Comparison dataframe created with {len(comparison_df)} values")
print(f"Correlation between revealed and stated: {comparison_df['revealed_elo'].corr(comparison_df['stated_elo']):.3f}\n")


# ============================================================================
# STEP 2: CREATE SIDE-BY-SIDE COMPARISON CHART
# ============================================================================

print("Creating side-by-side comparison chart...")

# Prepare data for grouped bar chart
plot_df = comparison_df.copy()
plot_df['value_short'] = plot_df['value']  # Can truncate if names are too long

# Create bar chart using plotly express
fig_comparison = px.bar(
    plot_df,
    x='value',
    y=['revealed_elo', 'stated_elo'],
    barmode='group',
    title=f"{model}: Revealed vs Stated Value Preferences",
    labels={'value': 'Value Class', 'value': 'Elo Rating'},
    color_discrete_map={'revealed_elo': '#1f77b4', 'stated_elo': '#ff7f0e'}
)

# Update layout
fig_comparison.update_layout(
    xaxis_title="Value Class",
    yaxis_title="Elo Rating",
    legend_title="Preference Type",
    height=600,
    width=1200,
    xaxis_tickangle=-45,
    legend=dict(
        yanchor="top",
        y=0.99,
        xanchor="right",
        x=0.99
    )
)

# Update legend labels
fig_comparison.for_each_trace(lambda t: t.update(name='Revealed (Behavioral)' if t.name == 'revealed_elo' else 'Stated (Self-Report)'))

# Save
fig_comparison.write_image(comparison_chart_path)
print(f"  ✓ Saved to: {comparison_chart_path}\n")


# ============================================================================
# STEP 3: CREATE SCATTER PLOT (CORRELATION)
# ============================================================================

print("Creating correlation scatter plot...")

# Create scatter plot
fig_scatter = px.scatter(
    comparison_df,
    x='revealed_elo',
    y='stated_elo',
    text='value',
    title=f"{model}: Correlation Between Revealed and Stated Preferences",
    labels={'revealed_elo': 'Revealed Elo (Behavioral)', 'stated_elo': 'Stated Elo (Self-Report)'}
)

# Add diagonal line (perfect correlation)
min_val = min(comparison_df['revealed_elo'].min(), comparison_df['stated_elo'].min())
max_val = max(comparison_df['revealed_elo'].max(), comparison_df['stated_elo'].max())
fig_scatter.add_shape(
    type='line',
    x0=min_val, y0=min_val,
    x1=max_val, y1=max_val,
    line=dict(color='gray', dash='dash'),
    name='Perfect Correlation'
)

# Position text labels
fig_scatter.update_traces(textposition='top center', textfont_size=8)

# Update layout
fig_scatter.update_layout(
    height=700,
    width=700,
    showlegend=True
)

# Save
fig_scatter.write_image(scatter_plot_path)
print(f"  ✓ Saved to: {scatter_plot_path}\n")


# ============================================================================
# STEP 4: CREATE DIFFERENCE CHART
# ============================================================================

print("Creating difference chart...")

# Sort by difference for this chart
diff_df = comparison_df.sort_values('difference', ascending=True)

# Create bar chart showing differences
fig_diff = px.bar(
    diff_df,
    x='value',
    y='difference',
    title=f"{model}: Stated vs Revealed Preference Gaps",
    labels={'difference': 'Elo Difference (Stated - Revealed)', 'value': 'Value Class'},
    color='difference',
    color_continuous_scale='RdBu_r',  # Red for negative, blue for positive
    color_continuous_midpoint=0
)

# Add reference line at 0
fig_diff.add_hline(y=0, line_dash="dash", line_color="black", opacity=0.5)

# Update layout
fig_diff.update_layout(
    xaxis_title="Value Class",
    yaxis_title="Elo Difference (Stated - Revealed)",
    height=600,
    width=1200,
    xaxis_tickangle=-45,
    showlegend=False
)

# Add annotations for interpretation
fig_diff.add_annotation(
    text="Positive = Says more important than acts",
    xref="paper", yref="paper",
    x=0.02, y=0.98,
    showarrow=False,
    bgcolor="rgba(255,255,255,0.8)",
    bordercolor="blue",
    borderwidth=1
)
fig_diff.add_annotation(
    text="Negative = Acts more important than says",
    xref="paper", yref="paper",
    x=0.02, y=0.02,
    showarrow=False,
    bgcolor="rgba(255,255,255,0.8)",
    bordercolor="red",
    borderwidth=1
)

# Save
fig_diff.write_image(difference_chart_path)
print(f"  ✓ Saved to: {difference_chart_path}\n")


# ============================================================================
# STEP 5: CREATE RANK COMPARISON CHART
# ============================================================================

print("Creating rank comparison chart...")

# Sort by revealed rank for this chart (to match revealed preference order)
rank_df = comparison_df.sort_values('revealed_rank')

# Create figure with connected lines showing rank changes
import plotly.graph_objects as go

fig_rank = go.Figure()

# Add lines connecting revealed rank to stated rank for each value
for _, row in rank_df.iterrows():
    # Determine color based on rank change
    if row['rank_change'] > 0:
        # Moved up in stated (blue - became MORE important when asked)
        line_color = 'rgba(31, 119, 180, 0.6)'
    elif row['rank_change'] < 0:
        # Moved down in stated (red - became LESS important when asked)
        line_color = 'rgba(255, 127, 14, 0.6)'
    else:
        # No change (gray)
        line_color = 'rgba(150, 150, 150, 0.6)'

    fig_rank.add_trace(go.Scatter(
        x=['Revealed', 'Stated'],
        y=[row['revealed_rank'], row['stated_rank']],
        mode='lines+markers+text',
        line=dict(color=line_color, width=2),
        marker=dict(size=8),
        text=['', row['value']],  # Label only on the stated side
        textposition='middle right',
        textfont=dict(size=9),
        hovertemplate=f"<b>{row['value']}</b><br>" +
                      f"Revealed Rank: {row['revealed_rank']}<br>" +
                      f"Stated Rank: {row['stated_rank']}<br>" +
                      f"Change: {row['rank_change']:+d}<extra></extra>",
        showlegend=False
    ))

# Update layout
fig_rank.update_layout(
    title=f"{model}: Rank Order Comparison (Revealed vs Stated Preferences)",
    xaxis=dict(
        title="Preference Type",
        tickmode='array',
        tickvals=['Revealed', 'Stated'],
        ticktext=['Revealed<br>(Behavioral)', 'Stated<br>(Self-Report)']
    ),
    yaxis=dict(
        title="Rank (1 = Most Important)",
        autorange='reversed',  # Rank 1 at top
        tickmode='linear',
        tick0=1,
        dtick=1
    ),
    height=800,
    width=800,
    hovermode='closest'
)

# Add annotations for interpretation
fig_rank.add_annotation(
    text="Lines moving DOWN (revealed→stated) = Value ranks HIGHER when asked directly",
    xref="paper", yref="paper",
    x=0.5, y=1.05,
    showarrow=False,
    font=dict(size=11),
    xanchor='center'
)

# Save
fig_rank.write_image(rank_comparison_path)
print(f"  ✓ Saved to: {rank_comparison_path}\n")


# ============================================================================
# STEP 6: DISPLAY SUMMARY STATISTICS
# ============================================================================

print(f"{'='*80}")
print(f"REVEALED vs STATED PREFERENCES COMPARISON")
print(f"Model: {model}")
print(f"{'='*80}\n")

# Overall correlation (Pearson - measures linear relationship between Elo values)
pearson_correlation = comparison_df['revealed_elo'].corr(comparison_df['stated_elo'])
print(f"Pearson Correlation (Elo values): {pearson_correlation:.3f}")
print(f"  (1.0 = perfect linear agreement, 0.0 = no relationship, -1.0 = opposite)")

# Spearman's rho correlation (measures monotonic relationship between ranks)
spearman_correlation, spearman_pvalue = spearmanr(comparison_df['revealed_rank'], comparison_df['stated_rank'])
print(f"\nSpearman's Rho (Rank correlation): {spearman_correlation:.3f}")
print(f"  (p-value: {spearman_pvalue:.4f})")
print(f"  (1.0 = perfect rank agreement, 0.0 = no relationship, -1.0 = opposite)\n")

# Values with biggest positive gaps (say more important than act)
print("Top 5 Values: STATED > REVEALED (Says more important than acts)")
print("-" * 80)
top_positive = comparison_df.nlargest(5, 'difference')[['value', 'stated_elo', 'revealed_elo', 'difference']]
print(top_positive.to_string(index=False))
print()

# Values with biggest negative gaps (act more important than say)
print("Top 5 Values: REVEALED > STATED (Acts more important than says)")
print("-" * 80)
top_negative = comparison_df.nsmallest(5, 'difference')[['value', 'stated_elo', 'revealed_elo', 'difference']]
print(top_negative.to_string(index=False))
print()

# Values with smallest gaps (most consistent)
print("Top 5 Most Consistent Values (Smallest gap between stated and revealed)")
print("-" * 80)
most_consistent = comparison_df.nsmallest(5, 'abs_difference')[['value', 'stated_elo', 'revealed_elo', 'difference']]
print(most_consistent.to_string(index=False))
print()

# Values with biggest rank changes
print("Top 5 Values with Biggest Rank INCREASES in Stated Preferences")
print("-" * 80)
print("(Negative rank_change = ranked higher when asked directly)")
biggest_increase = comparison_df.nsmallest(5, 'rank_change')[['value', 'revealed_rank', 'stated_rank', 'rank_change']]
print(biggest_increase.to_string(index=False))
print()

print("Top 5 Values with Biggest Rank DECREASES in Stated Preferences")
print("-" * 80)
print("(Positive rank_change = ranked lower when asked directly)")
biggest_decrease = comparison_df.nlargest(5, 'rank_change')[['value', 'revealed_rank', 'stated_rank', 'rank_change']]
print(biggest_decrease.to_string(index=False))
print()

print(f"{'='*80}")
print(f"Visualizations saved:")
print(f"  1. Side-by-side comparison: {comparison_chart_path}")
print(f"  2. Correlation scatter plot: {scatter_plot_path}")
print(f"  3. Difference chart: {difference_chart_path}")
print(f"  4. Rank comparison: {rank_comparison_path}")
print(f"{'='*80}\n")
